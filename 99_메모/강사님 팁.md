### 1. 하나의 파일에서 작업을 다 하려고 하지 마세요

- jupyter notebook 하나에 모든 작업을 몰아넣지 말라는 의미
- 데이터를 읽고, 전처리하고, 모델링하고, 평가하고, 제춢 파일까지 만드는 전 과정을 하나의 ipynb 파일에 작성하면 가독성이 떨어지고, 관리가 어렵고, 디버깅이 힘들어지기 때문임.

```jsx
📁 project/
├── 01_load_and_clean.ipynb       # 데이터 로딩 및 전처리
├── 02_eda.ipynb                  # 탐색적 데이터 분석 (EDA)
├── 03_modeling.ipynb            # 모델 학습
├── 04_inference_submission.ipynb # 추론 및 제출 파일 생성
└── data/
    ├── raw/                      # 원본 데이터
    ├── processed/                # 전처리된 데이터
```

### 2. 데이터를 읽어온 다음 데이터를 가공했다면 가공한 데이터 파일로 저장하고 새로운 ipynb 파일을 만들어서 저장한 데이터를 읽어와 작업하는 형태로 해주세요.

- 예를 들어 `train.csv` 를 불러와 전처리를 수행한 후, 그 전처리된 데이터를 `parquet`이나 `csv` 파일로 저장하고 모델링 단계에서 `train_processed.parquet`을 불러와 시작하라는 의미임.
- 재현성 확보와 메모리 사용 최적화(불필요한 재전처리 방지), 프로젝트 모듈화(각 단계가 독립적으로 실행 가능하도록)를 위해서

### 3. 메모리 관리 등이 심사기준에 포함됨

- 단순히 정확도만 보는 것이 아니라, 코드 구조/효율성, 메모리 사용 최적화, 파일 포맷 사용 여부도 평가기준에 포함된다는 뜻
- 특히 parquet은
    - 메모리 효율이 뛰어나고
    - 빠르게 읽기 가능하며
    - pandas에서도 잘 지원되는 형식임.

### 4. sample_submission : 제출 양식

- 이 파일은 제출 시 필요한 형식을 보여주는 템플릿
- 이 구조를 그대로 맞춰서 제출해야 평가 시스템에서 인식함.

### 5. 신용카드 고객 세그먼트 분류 AI 경진대회

- classification 문제
- 고객의 다양한 속성 정보 (소득, 소비, 패턴, 나이 등)를 기반으로 어떤 세그먼트에 속하는지를 분류하는 문제
- train 데이터에는 `segment` (정답 라벨)가 존재하므로 학습에 사용
- test 데이터는 `segment` 가 없기 때문에 학습된 모델을 바탕으로 예측해야 함.

### 6. 데이터 명세서랑 한번 맞춰보면서 segment를 나누는 게 무엇인지 상관관계를 분석해보세요

- segment가 어떻게 정의되어 있는지 명확하지 않을 수 있으므로, 데이터 명세서를 참고하면서 다음을 수행
    - 어떤 변수들이 segment와 관계가 있는지 분석
    - 상관관계, 시각화, 클러스터링 등을 통해 분류 기준 추

| 단계 | 파일 이름 | 주요 작업 |
| --- | --- | --- |
| 1 | `01_load_and_clean.ipynb` | 원본 데이터 로딩, 결측치 처리, 전처리 후 parquet 저장 |
| 2 | `02_eda.ipynb` | 데이터 시각화, 변수 간 관계 파악, 세그먼트 특징 분석 |
| 3 | `03_modeling.ipynb` | 모델 학습, 교차 검증, 하이퍼파라미터 튜닝 |
| 4 | `04_inference_submission.ipynb` | test 데이터 불러오기, 예측, 제출 파일 생성 |

### 7. XGBoost

| 항목 | 요약 |
| --- | --- |
| ✅ XGBoost | GPU 설정 없이도 쉽게 사용 가능. 단, GPU 성능을 쓰고 싶다면 `device='cuda'`만 지정하면 됨 |
| ✅ CUDA | 딥러닝처럼 복잡한 CUDA 설정 필요 없음. 그래픽 카드 없으면 Google Colab 추천 |
| ✅ 하이퍼파라미터 튜닝 | 복잡하게 하지 않아도 됨. `XGBClassifier` 기본값으로도 충분히 성능이 나옴 |
| ✅ 교차검증 | `cross_val_score`로 10겹 교차검증 진행 |
| ✅ 중요한 것 | 모델 튜닝보다 **EDA(데이터 분석)** 및 **변수 간 상관관계 이해**가 핵심 |

---

## 🧠 작업 흐름 요약

### 1. **데이터 분석(EDA) 최우선**

- `segment`(A~E)가 어떤 의미를 가지는지 추론
- 변수별로 `segment`와의 상관관계 시각화
- `groupby('segment')`로 평균값 비교, boxplot, heatmap 활용

### 2. **모델 학습은 XGBoost 한 방이면 충분**

- `train_X`, `train_y`만 잘 구성되면 됨
- 복잡한 파라미터 튜닝이나 딥러닝 모델링은 필요 없음
- 코드 예시:

```python

from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, KFold
import time

start = time.time()
model = XGBClassifier(tree_method='hist', device='cuda')  # device='cuda' 생략 시 CPU

kfold = KFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(model, train_X, train_y, scoring='f1_macro', cv=kfold)
print(f"평균 F1 Score: {scores.mean():.4f}")
print(f"{time.time() - start:.2f} sec")

```

---

## 💡 지금 해야 할 작업 제안

| 순서 | 작업 내용 | 방법 |
| --- | --- | --- |
| 1 | 업로드한 `회원정보.csv`에서 `segment` 별로 특성 비교 | `df.groupby('segment').mean()` |
| 2 | 변수별 boxplot 시각화로 segment별 차이 보기 | `sns.boxplot(x='segment', y='신용점수', data=df)` |
| 3 | 연속형/범주형 변수별 상관관계 heatmap | `df.corr(numeric_only=True)` |
| 4 | 위에서 추출한 변수들을 기준으로 `train_X`, `train_y` 생성 | X = 주요 변수, y = segment |
| 5 | 위 코드로 XGBoost 모델 학습 및 평가 | 그대로 복붙해서 실행 가능 |

---

## ✨ 결론

> “머신러닝은 전에 만들었던 코드를 붙여놓고 돌리면 됩니다. 중요한 것은 데이터를 얼마나 잘 이해했는가.”
> 

**세그먼트를 잘 분석해서**,

→ **모델에 넣을 좋은 변수(feature)를 고르고**,

→ **XGBoost로 예측**
