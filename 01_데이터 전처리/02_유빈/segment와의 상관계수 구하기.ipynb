{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613b683c-6d94-43a8-a150-df4ce1ee5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# 그래프 기본 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 경고 뜨지 않게\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#저장라이브러리\n",
    "import pickle\n",
    "\n",
    "# 평가함수\n",
    "# 분류용\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 회귀용\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 데이터를 학습용과 검증용으로 나눈다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "\n",
    "# 딥러닝 모델 구조를 정의하는 것\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 층구조를 정의하는 것\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 활성화 함수를 정의하는 것\n",
    "from tensorflow.keras.layers import Activation\n",
    "# Convolutional Layer \n",
    "# 합성곱을 수행하는 레이어\n",
    "# Convolutionla Layer 를 사용하는 신경망을 CNN이라고 부른다.\n",
    "# 커널에 설정되어 있는 가중치 값에 따라서 특정 부분의 값을 낮추고 특정 부분의 값을 키워서\n",
    "# 특성이 잘 드러나게 하는 역할을 수행한다.\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "# MaxPooling\n",
    "# Convolutional Layer를 통과한 데이터에서 큰 부분만 취하는 역할을 수행한다.\n",
    "# 필요한 특성을 나타내는 부분을 취하고 나머지는 버리는 효과를 얻게 된다.\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "# 다차원의 데이터를 1차원 데이터로 변환시켜준다.\n",
    "# 선형회귀 레이어로 가기전에 두어야 한다.\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# Dropout : 데이터 하나가 들어오면 레이어의 모든 노드가 그 데이터를 학습하게 된다.\n",
    "# 그렇기 때문에 모든 가중치 값이 데이터에 영향을 받아서 변경된다.\n",
    "# 만약 각 노드들이 특정 패턴의 데이터만 학습하고자 한다면 학습시 일부 노드들을 비활성화 시켜서\n",
    "# 학습에 참여시키지 않게 할 수 있다.\n",
    "# 전체 데이터에 대해 모든 노드들이 과접합을 되는 것을 예방한다.\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# Embedding : 주어진 단어 데이터를 통해 단어 벡터를 생성하는 레이어\n",
    "# 생성한 벡터 공간의 차원을 매개변수로 받는다.\n",
    "# 주어진 문장 데이터의 단어 개수가 공간의 차원을 넘어서면 출현 빈도수가 높은 단어들로만\n",
    "# 구성하여 공간 벡터를 구성한다.\n",
    "from tensorflow.keras.layers import Embedding\n",
    "# LSTM : RNN 용 Layer\n",
    "# 이전에 입력된 데이터와 함께 묶어서 학습할 데이터를 생성하는 방식으로 동작하는 레이어\n",
    "# 순환신경망. 문장 데이터, 주식시세, 시계열 데이터 등의 데이터 패턴을 보고 다음 데이터를\n",
    "# 예측하는데 사용한다.\n",
    "# LSTM은 Long Term Memory와 Short Term Memory를 두어 데이터의 패턴이 좀더 다양하게 생성될 수 \n",
    "# 있도록 하는 특징을 가지고 있다.\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# GAN\n",
    "# GAN에서 사용하는 활성화 함수\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "# 레이어를 통과한 데이터를 다시 표준화하여 학습 효율성을 높히게 한다.\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# 레이어를 통과한 데이터의 차원을 변경하기위 사용한다.\n",
    "from tensorflow.keras.layers import Reshape\n",
    "# 데이터 업샘플링 - Convolutional Layer가 데이터의 크기를 줄이므로 늘리는 것이 필요하다\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "# 입력층\n",
    "from tensorflow.keras.layers import Input\n",
    "# 딥러닝 모델, GAN은 생성자 신경망과 판별자 신경망을 같이 써야 하기 때문에\n",
    "# 이 둘을 하나의 신경망으로 합치기 위해 사용한다.\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 다중 분류를 위한 원핫 인코딩\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 저장된 학습 모델 복원\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# epoch마다 모델을 저장하는 함수\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# 조기 중단\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 경사하강법\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 문자열을 잘라낸다.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# 모든 문장 데이터의 단어 데이터 수를 동일하게 맞춰준다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 문자열을 단어 사전으로 만들어준다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 전이 학습을 위한 사전 학습 모델\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# 이미지 데이터를 불러올 때 나눠서 읽어오거나 읽어올때 이미지 데이터를 가공하면서\n",
    "# 가져울 수 있는 도구\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 출력한 것을 청소하는 함수\n",
    "from IPython.display import clear_output\n",
    "# 시간 관련\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# gpu가 있다면..\n",
    "if len(gpus) > 0 :\n",
    "    try :\n",
    "        for gpu in gpus :\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e :\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7daf23-e751-40ba-a998-b2c35f81c928",
   "metadata": {},
   "source": [
    "### 원하는 컬럼을 넣고 돌리면 상관계수가 나온오게 하는 코드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a457a709-3378-4382-922e-1818c3ee4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "홈페이지_금융건수_R6M 와 Segment 상관계수: -0.169\n",
      "홈페이지_선결제건수_R6M 와 Segment 상관계수: -0.151\n",
      "홈페이지_금융건수_R3M 와 Segment 상관계수: -0.160\n",
      "홈페이지_선결제건수_R3M 와 Segment 상관계수: -0.149\n",
      "당사멤버쉽_방문횟수_B0M 와 Segment 상관계수: -0.090\n",
      "당사멤버쉽_방문횟수_R6M 와 Segment 상관계수: -0.098\n",
      "당사멤버쉽_방문월수_R6M 와 Segment 상관계수: -0.133\n",
      "OS구분코드 컬럼은 문자열이므로 상관계수 계산 불가\n"
     ]
    }
   ],
   "source": [
    "# 1. 비교할 컬럼 리스트\n",
    "columns_to_compare = [\n",
    "    '홈페이지_금융건수_R6M',\n",
    "    '홈페이지_선결제건수_R6M',\n",
    "    '홈페이지_금융건수_R3M',\n",
    "    '홈페이지_선결제건수_R3M',\n",
    "    '당사멤버쉽_방문횟수_B0M',\n",
    "    '당사멤버쉽_방문횟수_R6M',\n",
    "    '당사멤버쉽_방문월수_R6M',\n",
    "    'OS구분코드'\n",
    "]\n",
    "\n",
    "# 2. 파일 불러오기 (Segment는 '1.회원정보'에서, 잔액은 '5.잔액정보'에서)\n",
    "df_member = pd.read_parquet(\n",
    "    'open/train/1.회원정보',\n",
    "    columns=['ID', 'Segment']\n",
    ")\n",
    "\n",
    "df_balance = pd.read_parquet(\n",
    "    'open/train/6.채널정보',\n",
    "    columns=['ID'] + columns_to_compare\n",
    ")\n",
    "\n",
    "# 3. Segment 문자 → 숫자 인코딩\n",
    "le = LabelEncoder()\n",
    "df_member['Segment_code'] = le.fit_transform(df_member['Segment'])\n",
    "\n",
    "# 4. 병합\n",
    "df = pd.merge(df_member[['ID', 'Segment_code']], df_balance, on='ID')\n",
    "\n",
    "# 5. 상관계수 계산\n",
    "for col in columns_to_compare:\n",
    "    if col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            subset = df[['Segment_code', col]].dropna()\n",
    "            if subset[col].nunique() > 1:\n",
    "                corr = subset.corr().iloc[0, 1]\n",
    "                print(f\"{col} 와 Segment 상관계수: {corr:.3f}\")\n",
    "            else:\n",
    "                print(f\"{col} 컬럼은 상수이므로 상관계수 계산 불가\")\n",
    "        else:\n",
    "            print(f\"{col} 컬럼은 문자열이므로 상관계수 계산 불가\")\n",
    "    else:\n",
    "        print(f\"{col} 컬럼이 데이터에 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf25db48-8c52-4a2b-a15a-406c594d9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이용금액대 와 Segment 상관계수: -0.484\n"
     ]
    }
   ],
   "source": [
    "# 1. 비교할 컬럼 리스트\n",
    "columns_to_compare = [\n",
    "    '이용금액대'\n",
    "]\n",
    "\n",
    "# 2. 데이터 로딩 (필요 컬럼만)\n",
    "df_member = pd.read_parquet(\n",
    "    'open/train/1.회원정보',\n",
    "    columns=['ID', 'Segment']\n",
    ")\n",
    "\n",
    "df_sales = pd.read_parquet(\n",
    "    'open/train/5.잔액정보',\n",
    "    columns=['ID'] + columns_to_compare\n",
    ")\n",
    "\n",
    "# 3. Segment 인코딩\n",
    "le = LabelEncoder()\n",
    "df_member['Segment_code'] = le.fit_transform(df_member['Segment'])\n",
    "\n",
    "# 4. 병합\n",
    "df = pd.merge(df_member[['ID', 'Segment_code']], df_sales, on='ID')\n",
    "\n",
    "# 5. '이용금액대' 매핑 (순서형 수치로)\n",
    "amount_mapping = {\n",
    "    '05.10만원-': 1,\n",
    "    '04.10만원+': 2,\n",
    "    '03.30만원+': 3,\n",
    "    '02.50만원+': 4,\n",
    "    '01.100만원+': 5,\n",
    "    '09.미사용': np.nan  # 미사용은 상관계수 분석에서 제외\n",
    "}\n",
    "df['이용금액대_코드'] = df['이용금액대'].map(amount_mapping)\n",
    "\n",
    "# 6. 상관계수 계산\n",
    "for col in columns_to_compare:\n",
    "    target_col = '이용금액대_코드' if col == '이용금액대' else col\n",
    "\n",
    "    if target_col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "            subset = df[['Segment_code', target_col]].dropna()\n",
    "            if subset[target_col].nunique() > 1:\n",
    "                corr = subset.corr().iloc[0, 1]\n",
    "                print(f\"{col} 와 Segment 상관계수: {corr:.3f}\")\n",
    "            else:\n",
    "                print(f\"{col} 컬럼은 값이 모두 같아서 상관계수 계산 불가\")\n",
    "        else:\n",
    "            print(f\"{col} 컬럼은 숫자형이 아니라 상관계수 계산 불가 (문자형)\")\n",
    "    else:\n",
    "        print(f\"{col} 컬럼이 데이터에 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390136fc-ee24-4d63-a20c-ad59254b5a3d",
   "metadata": {},
   "source": [
    "### 컬럼 내부 분류기준이 정확하지 않을 때 명칭을 조사한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c4f6ca3-f982-4fed-9460-796318295476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.100만원+' '03.30만원+' '09.미사용' '04.10만원+' '02.50만원+' '05.10만원-']\n"
     ]
    }
   ],
   "source": [
    "df_amount_range = pd.read_parquet(\n",
    "    'open/train/3.승인매출정보',\n",
    "    columns=['ID', '이용금액대']  # ID도 같이 가져오면 나중에 병합 편함\n",
    ")\n",
    "\n",
    "# 고유값 확인\n",
    "print(df_amount_range['이용금액대'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106484ea-e242-44ad-9177-1ed1dac73c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
